import os
import datetime
from evaluation.run_eval import evaluate_predictions
# from util import print_msg


class TranslationEvaluator:
    def __init__(self, config, output_dir):
        """
        TranslationEvaluator that will handle the evaluation of
        BLEU-1,2,3,4 and ROUGE-L
        """
        self.output_dir = os.path.join(output_dir, "result")
        if not os.path.exists(self.output_dir):
            os.makedirs(self.output_dir)
        if config is None:
            self.timestamp = datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
        else:
            self.config = config
            self.timestamp = self.config.timestamp
        self.name = "TranslationEvaluator"
        self.beam_size = self.config.beam_size

        # Three List of Strings to contains all the necessary logging
        self.model_inputs = []
        self.model_outputs = []
        self.ground_truths = []

    def evaluate_score(self, name="train"):
        """
        Evaluate the item in self.model_outputs and self.ground_truths
        :return:
        """
        assert len(self.model_outputs) == len(self.ground_truths) == len(self.model_inputs), "Evaluation Error: " \
                                                                                             "List not Equal"
        assert len(self.model_outputs) != 0, "Empty List"
        pred_out_path = os.path.join(self.output_dir, name + "_pred.txt")
        groundtruth_out_path = os.path.join(self.output_dir, name + "_groundtruth.txt")
        input_out_path = os.path.join(self.output_dir, name + "_input.txt")
        self._save_list_to_txt(self.model_inputs, input_out_path)
        # self.print_msg("Model Output saved to %s" % pred_out_path)
        self._save_list_to_txt(self.model_outputs, pred_out_path)
        # self.print_msg("Groundtruth saved to %s" % groundtruth_out_path)
        self._save_list_to_txt(self.ground_truths, groundtruth_out_path)
        metrics = evaluate_predictions(self.ground_truths, self.model_outputs)
        racc = self.compute_racc()
        metrics["RAcc"] = racc
        self.reset_list()
        return metrics

    @staticmethod
    def _save_list_to_txt(item_list, output_path):
        """
        Save the item in the item_list to output_path, with each item as a line
        :param item_list: A list of Items
        :param output_path: Path of the Output
        :return:
        """
        with open(output_path, 'w') as wfile:
            for item in item_list:
                wfile.write("%s\n" % item)

    def reset_list(self):
        """
        Reset the list for each epoch
        :return: NIL
        """
        self.model_inputs = []
        self.model_outputs = []
        self.ground_truths = []

    def add_strings(self, model_input, model_output, ground_truth):
        """
        Add the model item into our list to keep track of the items
        :param model_input: Input to the model, Source
        :param model_output: Output of the model, Target
        :param ground_truth: Ground Truth of the Data
        :return: NIL
        """
        self.model_inputs.extend(model_input)
        self.model_outputs.extend(model_output)
        self.ground_truths.extend(ground_truth)

    def compute_racc(self):
        """
        Compute Repair Accuracy based on Beam Size
        :return: Return the Average RAcc of the Dataset
        """
        raccs = []
        batch_model_outputs = []
        batch_ground_truths = []
        assert len(self.ground_truths) == len(self.model_outputs)
        for i in range(0, len(self.ground_truths), self.beam_size):
            batch_model_outputs.append(self.model_outputs[i:i+self.beam_size])
            batch_ground_truths.append(self.ground_truths[i])
        assert len(batch_ground_truths) == len(batch_model_outputs)
        for i in range(len(batch_ground_truths)):
            gt = batch_ground_truths[i]
            outputs = batch_model_outputs[i]  # All the outputs generated by model
            if gt in outputs:
                raccs.append(1)
            else:
                raccs.append(0)
        return sum(raccs) / float(len(raccs))
