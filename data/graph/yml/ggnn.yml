# Name of Configuration and Experiment Mode.
custom: True
name: "CodeClassificationConfig"
task: "CodeClassification"
model_type: "GGNN"
task_type: "Classification"
classify_mode: "Binary"

# Vocabulary Setting
token_vocab_path: "/Users/tom/Downloads/learning-program-representation-master/data/custom/graph/ast/dict/token_vocab_dict.pkl"
node_vocab_path: "/Users/tom/Downloads/learning-program-representation-master/data/custom/graph/ast/dict/node_vocab_dict.pkl"

# Dataset Specification Configuration
dataset:
  name: "custom"
  path: "/Users/tom/Downloads/learning-program-representation-master/data/custom/graph/ast/context-preserved"

# General Configuration
output_path: "./trained_model/code_classification/ggnn"
use_cuda: False
disable_tqdm: True
class_weight: True
initial_test: False
save_output: True
use_scheduler: True
mm_var: "accuracy"
monitor_vars: ["accuracy","recall","precision","f1","auc"]
gpu_id: 1

# Generic Model Hyperparameters
max_code_token_len: 20
class_num: 2
optimizer_type: ADAM # SGD, ADAMW, ADAMAX, ADAM
lr: 0.001
batch_size: 128
max_epoch: 500
patience: 100
word_emb_dims: 128
dropout: 0.2
self_loop: True
reverse_edge: True
use_edge_type: ddg # ast, cfg, cdg, ddg, and cpg


# Node Embedding Layer
node_emb_layer:
  mode: "LSTMEmbedNode" # AverageEmbedNode/LSTMEmbedNode/MLP
  dims: 128
  layers: 1
  use_nfeature: "both" # structure or textual or both-full-textual or both-id-textual


# GGNN Parameters
ggnn:
  initial_representation: True
  in_dim: 128
  out_dim: 128
  nsteps: 4
  dropout: 0.2
  graph_agg: max # sum, mean, max






